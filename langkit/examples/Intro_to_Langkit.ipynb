{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro to LangKit\n",
    "\n",
    "Table of Contents\n",
    "- [Install LangKit](#intro-to-langkit)\n",
    "- [Initialize LLM metrics](#Initialize-LLM-metrics)\n",
    "- [Hello, World!](#hello,-world!)\n",
    "- [Comparing Data](#comparing-data)\n",
    "- [Next Steps](#next-steps)\n",
    "\n",
    "\n",
    "Ok! let's install __langkit__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: you may need to restart the kernel to use updated packages.\n",
    "%pip install 'langkit[all]' -q"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LLM metrics\n",
    "LangKit provides a toolkit of metrics for LLM applications, lets initialize them!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langkit import llm_metrics # alternatively use 'light_metrics'\n",
    "import whylogs as why\n",
    "\n",
    "why.init(session_type='whylabs_anonymous')\n",
    "# Note, llm_metrics.init() downloads models so this is slow first time.\n",
    "schema = llm_metrics.init() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hello, World!\n",
    "In the below code we log a few example prompt/response pairs and send metrics to a WhyLabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Aggregated 15 rows into profile 'langkit-sample-chats-all'\n",
      "\n",
      "Visualize and explore this profile with one-click\n",
      "üîç https://hub.whylabsapp.com/resources/model-1/profiles?sessionToken=session-6tuzHg&profile=ref-JHJ75222R6VdaTge\n"
     ]
    }
   ],
   "source": [
    "from langkit.whylogs.samples import load_chats\n",
    "\n",
    "results = why.log(load_chats(), name=\"langkit-sample-chats-all\", schema=schema)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Data\n",
    "Things get more interesting when you can compare two sets of metrics from an LLM application. The power of gathering systematic telemetry over time comes from being able to see how these metrics change, or how two sets of profiles compare. Below we asked GPT for some positive words and then asked for negative words as part of these two toy examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Aggregated 5 lines into profile 'positive_chats', 5 lines into profile 'negative_chats'\n",
      "\n",
      "Visualize and explore the profiles with one-click\n",
      "üîç https://hub.whylabsapp.com/resources/model-1/profiles?sessionToken=session-6tuzHg&profile=ref-zMGVHP0ww3QrGUXS&profile=ref-Rfryq1IXRIzqKtFL\n",
      "\n",
      "Or view each profile individually\n",
      " ‚§∑ https://hub.whylabsapp.com/resources/model-1/profiles?profile=ref-zMGVHP0ww3QrGUXS&sessionToken=session-6tuzHg\n",
      " ‚§∑ https://hub.whylabsapp.com/resources/model-1/profiles?profile=ref-Rfryq1IXRIzqKtFL&sessionToken=session-6tuzHg\n"
     ]
    }
   ],
   "source": [
    "pos_chats = load_chats(\"pos\")\n",
    "neg_chats = load_chats(\"neg\")\n",
    "\n",
    "results_comparison = why.log(multiple={\"positive_chats\": pos_chats,\n",
    "                                       \"negative_chats\": neg_chats},\n",
    "                            schema=schema)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can explore and compare specific metrics, in this example we expect a large and obvious distribution drift in the sentiment scores on the response which you can see [here](https://hub.whylabsapp.com/resources/model-1/profiles?feature-highlight=response.sentiment_nltk&includeType=discrete&includeType=non-discrete&limit=30&offset=0&profile=ref-zMGVHP0ww3QrGUXS&profile=ref-Rfryq1IXRIzqKtFL&sessionToken=session-6tuzHg&sortModelBy=LatestAlert&sortModelDirection=DESC)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "If you see value in detecting changes in how your LLM application is behaving, you might take a look at some of our other examples showing how to monitor these metrics as a timeseries for an LLM application in production, or how to customize the metrics logged by using your own surrogate models or critic metrics.\n",
    "* check out the [examples](https://github.com/whylabs/langkit/tree/main/langkit/examples) folder for scenarios from \"Hello World!\"  to monitoring an LLM in production!\n",
    "* Learn more about the [features](https://github.com/whylabs/langkit#features) LangKit extracts out of the box.\n",
    "* Learn more about LangKit's [modules documentation](https://github.com/whylabs/langkit/blob/main/langkit/docs/modules.md)\n",
    "* Explore more on [WhyLabs](https://whylabs.ai/whylabs-free-sign-up?utm_source=github&utm_medium=referral&utm_campaign=langkit) and monitor your LLM application over time!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langkit-EeFODeF5-py3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
