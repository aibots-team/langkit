{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "saXx8dDyVLdT"
      },
      "source": [
        ">### üö© *Create a free WhyLabs account to complete this example!*<br>\n",
        ">*Did you know you can store, visualize, and monitor whylogs profiles with the [WhyLabs Observability Platform](https://whylabs.ai/whylabs-free-sign-up?utm_source=github&utm_medium=referral&utm_campaign=langkit)? Sign up for a [free WhyLabs account](https://whylabs.ai/whylogs-free-signup?utm_source=github&utm_medium=referral&utm_campaign=LLM_to_WhyLabs) to leverage the power of whylogs and WhyLabs together!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtOtEl2HVLdX"
      },
      "source": [
        "# Writing Profiles to WhyLabs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALFaz6r8VLdY"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/whylabs/LanguageToolkit/blob/main/langkit/examples/LLM_to_WhyLabs.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD1F6JKLVLdY"
      },
      "source": [
        "In this example, we'll show how to send your LLM metrics to your monitoring dashboard at WhyLabs Platform.\n",
        "We will:\n",
        "\n",
        "- Define environment variables with the appropriate Credentials and IDs\n",
        "- Log LLM prompts and responses into a profile\n",
        "- Use the WhyLabs Writer to send the profile to your Project at WhyLabs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM4jZBCnVLdZ"
      },
      "source": [
        "## Installing LangKit\n",
        "\n",
        "First, let's install __langkit__."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YoAjaXeVLda"
      },
      "outputs": [],
      "source": [
        "# Note: you may need to restart the kernel to use updated packages.\n",
        "%pip install langkit[all] -q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NROvY2urVLdb"
      },
      "source": [
        "## ‚úîÔ∏è Setting the Environment Variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIvdzBFgVLdc"
      },
      "source": [
        "In order to send our profile to WhyLabs, let's first set up an account. You can skip this if you already have an account and a model set up.\n",
        "\n",
        "We will need three pieces of information:\n",
        "\n",
        "- API tokens for the LLM and WhyLabs\n",
        "- Organization ID for WhyLabs\n",
        "- Dataset ID for WhyLabs\n",
        "\n",
        "Go to [https://whylabs.ai/free](https://whylabs.ai/whylabs-free-sign-up?utm_source=github&utm_medium=referral&utm_campaign=langkit) and grab a free account. You can follow along with the examples if you wish, but if you‚Äôre interested in only following this demonstration, you can go ahead and skip the quick start instructions.\n",
        "\n",
        "After that, you‚Äôll be prompted to create an API token. Once you create it, copy and store it locally. The second important information here is your org ID. Take note of it as well. After you get your API Token and Org ID, you can go to https://hub.whylabsapp.com/models to see your projects dashboard. You can create a new project and take note of it's ID (if it's a model project it will look like `model-xxxx`)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIgRLJBuVLdd"
      },
      "source": [
        "We'll now check if the required credentials are set as environment variables. In a production setting these would already be set as environment variables, but here we prompt you if they are missing. You can still run the example without these, but we won't use a live session with GPT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP46seTvVLdd"
      },
      "outputs": [],
      "source": [
        "from langkit.config import check_or_prompt_for_api_keys\n",
        "from langkit.openai import ChatLog, send_prompt\n",
        "from langkit.openai_wrapper import openai_logger\n",
        "\n",
        "check_or_prompt_for_api_keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZMUzBsF9VLde"
      },
      "source": [
        "## üìä Profiling the Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bmxm2EI-VLde"
      },
      "source": [
        "For demonstration, let's use some archived chat gpt response/prompts data from Hugging Face, or you can set the interactive parameter to true and interact with ChatGPT to see how it works in real time if you already have an openai api key."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0Fd7ApAVLde"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langkit.whylogs.rolling_logger import RollingLogger\n",
        "\n",
        "telemetry_agent = RollingLogger()\n",
        "\n",
        "INTERACTIVE = bool(os.getenv(\"OPENAI_API_KEY\")) # set to True to test out interacting with ChatGPT\n",
        "interactive_prompt = \"\"\n",
        "if INTERACTIVE:\n",
        "    print(f\"At any time input 'q' or anything that begins with q to quit. enter a question for the LLM\")\n",
        "    while True:\n",
        "        print()\n",
        "        interactive_prompt = input(\"ask chat gpt: \")\n",
        "        if interactive_prompt.startswith('q'):\n",
        "            break\n",
        "        response = send_prompt(interactive_prompt)\n",
        "        # use the agent to log a dictionary, these should be flat\n",
        "        # to get the best results, in this case we log the prompt and response text\n",
        "        telemetry_agent.log(response.to_dict())\n",
        "        print(response.to_dict(), flush=True)\n",
        "else:\n",
        "    from datasets import load_dataset\n",
        "\n",
        "    archived_chats = load_dataset('alespalla/chatbot_instruction_prompts', split=\"test\", streaming=True)\n",
        "    chats = iter(archived_chats)\n",
        "    for _ in range(100):\n",
        "      response = next(chats)\n",
        "      telemetry_agent.log(response)\n",
        "      print(response)\n",
        "    # lets output the dataset metadata from hugging face so we can see how to\n",
        "    # access some of the contained prompts and responses.\n",
        "    print(\"done profiling\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_u0kohHVLdf"
      },
      "outputs": [],
      "source": [
        "# In practice you can use context manager lifecycle events to automatically close\n",
        "# loggers, this helps trigger a write ahead of schedule to avoid truncating the last interval\n",
        "# data seen by the agent.\n",
        "telemetry_agent.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PNJz2D5VLdf"
      },
      "source": [
        "## üîç A Look on the Other Side"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXCrRRpwVLdg"
      },
      "source": [
        "Now, check your dashboard to verify everything went ok. At the __Profile__ tab, you should see something like this: (TBD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdIYiVKqVLdg"
      },
      "source": [
        "This should give you a quick way to look at how your extracted metrics on the text prompts and responses, and these can be monitored over time!"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.8.10 ('.venv': poetry)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "d39f874c9b8a97550ecbd783714b95e79c9b905449b34f44c40e3bf053b54b41"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}